{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5QF0aa5kOYCQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Flatten\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import initializers, regularizers, optimizers, layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# !pip install -q -U keras-tuner\n",
    "# import kerastuner as kt\n",
    "\n",
    "import IPython\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from IPython.display import display \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src import load_text, get_word_index, lemmatize_grammarize_text\n",
    "\n",
    "\n",
    "sns.set(context = 'notebook', style = 'whitegrid')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "my6ULQUUDphy"
   },
   "source": [
    "https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633\n",
    "\n",
    "GloVe embeddigns thanks to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJRkd15UI5wN"
   },
   "source": [
    "Thanks to https://keras.io/examples/nlp/pretrained_word_embeddings/ and Kefei Mo https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633 \n",
    "\n",
    "for the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vJ73328RLVIT"
   },
   "outputs": [],
   "source": [
    "df = load_text(sentences=True, grammarize=False)\n",
    "word_index = get_word_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Grade</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>grammarized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I had just gone to Chobot Space and Science Center</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i have just go to chobot space and science center</td>\n",
       "      <td>I had just gone to + chobot + space and + science + center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I got there I didnt even know one planet in our solar system</td>\n",
       "      <td>1.0</td>\n",
       "      <td>when i get there i didnt even know one planet in our solar system</td>\n",
       "      <td>+ when I got there I didnt even know one planet in our solar system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soon I learned all eight planets in our solar system</td>\n",
       "      <td>1.0</td>\n",
       "      <td>soon i learn all eight planet in our solar system</td>\n",
       "      <td>+ soon I learned all eight planets in our solar system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I learned what the first rocket ship looked like</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i learn what the first rocket ship look like</td>\n",
       "      <td>I learned what the first rocket ship looked like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I learned how to land a rocket ship</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i learn how to land a rocket ship</td>\n",
       "      <td>I learned how to land a rocket ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9378</th>\n",
       "      <td>Then you need to put the soil that you digd where the been is</td>\n",
       "      <td>2.0</td>\n",
       "      <td>then you need to put the soil that you digd where the be be</td>\n",
       "      <td>+ then you need to put the soil that you digd where the been is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9379</th>\n",
       "      <td>Lex go to step 4 : Then you put more soil if your been or seed wasent cover all with soil</td>\n",
       "      <td>2.0</td>\n",
       "      <td>lex go to step then you put more soil if your be or seed wasent cover all with soil</td>\n",
       "      <td>+ lex go to step 4 : + then you put more soil if your been or seed wasent cover all with soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9380</th>\n",
       "      <td>Lex go to step 5 : The last thing to do with your been or seed put water in the soil that you had in your cup thet sead your name</td>\n",
       "      <td>2.0</td>\n",
       "      <td>lex go to step the last thing to do with your be or seed put water in the soil that you have in your cup thet sead your name</td>\n",
       "      <td>+ lex go to step 5 : + the last thing to do with your been or seed put water in the soil that you had in your cup thet sead your name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9381</th>\n",
       "      <td>Then you wait some days for your plant could grow</td>\n",
       "      <td>2.0</td>\n",
       "      <td>then you wait some day for your plant could grow</td>\n",
       "      <td>+ then you wait some days for your plant could grow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9382</th>\n",
       "      <td>Now you now how to grow a plant</td>\n",
       "      <td>2.0</td>\n",
       "      <td>now you now how to grow a plant</td>\n",
       "      <td>+ now you now how to grow a plant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9136 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     Text  \\\n",
       "0                                                                                     I had just gone to Chobot Space and Science Center    \n",
       "1                                                                      When I got there I didnt even know one planet in our solar system    \n",
       "2                                                                                   Soon I learned all eight planets in our solar system    \n",
       "3                                                                                       I learned what the first rocket ship looked like    \n",
       "4                                                                                                    I learned how to land a rocket ship    \n",
       "...                                                                                                                                   ...   \n",
       "9378                                                                       Then you need to put the soil that you digd where the been is    \n",
       "9379                                           Lex go to step 4 : Then you put more soil if your been or seed wasent cover all with soil    \n",
       "9380   Lex go to step 5 : The last thing to do with your been or seed put water in the soil that you had in your cup thet sead your name    \n",
       "9381                                                                                   Then you wait some days for your plant could grow    \n",
       "9382                                                                                                     Now you now how to grow a plant    \n",
       "\n",
       "      Grade  \\\n",
       "0       1.0   \n",
       "1       1.0   \n",
       "2       1.0   \n",
       "3       1.0   \n",
       "4       1.0   \n",
       "...     ...   \n",
       "9378    2.0   \n",
       "9379    2.0   \n",
       "9380    2.0   \n",
       "9381    2.0   \n",
       "9382    2.0   \n",
       "\n",
       "                                                                                                                        lemmatized  \\\n",
       "0                                                                                i have just go to chobot space and science center   \n",
       "1                                                                when i get there i didnt even know one planet in our solar system   \n",
       "2                                                                                soon i learn all eight planet in our solar system   \n",
       "3                                                                                     i learn what the first rocket ship look like   \n",
       "4                                                                                                i learn how to land a rocket ship   \n",
       "...                                                                                                                            ...   \n",
       "9378                                                                   then you need to put the soil that you digd where the be be   \n",
       "9379                                           lex go to step then you put more soil if your be or seed wasent cover all with soil   \n",
       "9380  lex go to step the last thing to do with your be or seed put water in the soil that you have in your cup thet sead your name   \n",
       "9381                                                                              then you wait some day for your plant could grow   \n",
       "9382                                                                                               now you now how to grow a plant   \n",
       "\n",
       "                                                                                                                                  grammarized  \n",
       "0                                                                                 I had just gone to + chobot + space and + science + center   \n",
       "1                                                                        + when I got there I didnt even know one planet in our solar system   \n",
       "2                                                                                     + soon I learned all eight planets in our solar system   \n",
       "3                                                                                           I learned what the first rocket ship looked like   \n",
       "4                                                                                                        I learned how to land a rocket ship   \n",
       "...                                                                                                                                       ...  \n",
       "9378                                                                         + then you need to put the soil that you digd where the been is   \n",
       "9379                                           + lex go to step 4 : + then you put more soil if your been or seed wasent cover all with soil   \n",
       "9380   + lex go to step 5 : + the last thing to do with your been or seed put water in the soil that you had in your cup thet sead your name   \n",
       "9381                                                                                     + then you wait some days for your plant could grow   \n",
       "9382                                                                                                       + now you now how to grow a plant   \n",
       "\n",
       "[9136 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_grammarize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aCd8h46Dphy"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "\n",
    "num_words = len(word_index.keys())\n",
    "print(f'total vocabulary length: {num_words}')\n",
    "\n",
    "\n",
    "num_tokens = num_words + 1\n",
    "embedding_dim = len(nlp('the').vector)\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "\n",
    "    try:\n",
    "        embedding_matrix[i+1] = nlp(word).vector\n",
    "        hits += 1\n",
    "    except:\n",
    "        misses +=1\n",
    "print(f'words converted: {hits}, words not found: {misses}')\n",
    "tokens = df.Text.apply(lambda text: [word_index[word] for word in text.split()])\n",
    "X = pad_sequences(tokens, padding='post')\n",
    "y = df.Grade\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNnFthDvLVIU"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AV0p78v8ekvq"
   },
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False)\n",
    "version = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qWnjZGsDph0"
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))    \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=.01, decay=1e-2)\n",
    "    model.compile(optimizer = adam, loss = 'mean_absolute_error', metrics = None)\n",
    "    \n",
    "    return model\n",
    "model = make_model()\n",
    "filepath = 'model1-best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_k2FvshDph0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                     batch_size=100,\n",
    "                     epochs=50,\n",
    "                     validation_split=.2,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model = keras.models.load_model('model1-best.hdf5')\n",
    "yhat = model.predict(X_test).ravel()\n",
    "print('MAE = ', np.sum(np.abs(y_test-yhat))/len(y_test))\n",
    "print('mean grade prediction = ',np.mean(model.predict(X_train)))\n",
    "print('mean grade = ', np.mean(y_train))\n",
    "\n",
    "errors = df.loc[y_test.index][['Text','Grade']]\n",
    "errors['Predicted Grade'] = yhat\n",
    "errors.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoBoQ_8-nUFx"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eussx4uJdr_T"
   },
   "outputs": [],
   "source": [
    "def make_model2():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4))) \n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=.01, decay=1e-3)\n",
    "    model.compile(optimizer = adam, loss = 'mean_absolute_error', metrics = None)\n",
    "    \n",
    "    return model\n",
    "model = make_model2()\n",
    "print(model.summary())\n",
    "filepath = 'model2-best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                     batch_size=100,\n",
    "                     epochs=50,\n",
    "                     validation_split=.2,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model = keras.models.load_model('/content/model2-best.hdf5')\n",
    "yhat = model.predict(X_test).ravel()\n",
    "print('MAE = ', np.sum(np.abs(y_test-yhat))/len(y_test))\n",
    "print('mean grade prediction = ',np.mean(model.predict(X_train)))\n",
    "print('mean grade = ', np.mean(y_train))\n",
    "\n",
    "errors = df.loc[y_test.index][['Text','Grade']]\n",
    "errors['Predicted Grade'] = yhat\n",
    "errors.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7k1n-QtboPx"
   },
   "outputs": [],
   "source": [
    "def make_model3():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4))) \n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=.01, decay=1e-3)\n",
    "    model.compile(optimizer = adam, loss = 'mean_absolute_error', metrics = None)\n",
    "    \n",
    "    return model\n",
    "model = make_model3()\n",
    "print(model.summary())\n",
    "filepath = 'model3-best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                      batch_size=100,\n",
    "#                      epochs=200,\n",
    "#                      validation_split=.2,\n",
    "#                     callbacks=callbacks)\n",
    "\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "model = keras.models.load_model('/content/model3-best.hdf5')\n",
    "yhat = model.predict(X_test).ravel()\n",
    "print('MAE = ', np.sum(np.abs(y_test-yhat))/len(y_test))\n",
    "print('mean grade prediction = ',np.mean(model.predict(X_test)))\n",
    "print('mean grade = ', np.mean(y_test))\n",
    "\n",
    "errors = df.loc[y_test.index][['Text','Grade']]\n",
    "errors['Predicted Grade'] = yhat\n",
    "errors.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Clu57TeAqQwI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmckD6uiDbfM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GloVe_RNN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
