{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "5QF0aa5kOYCQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\caell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\caell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\caell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import plot_model\n",
    "\n",
    "!pip install -q -U keras-tuner\n",
    "import kerastuner as kt\n",
    "\n",
    "import IPython\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from IPython.display import display \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sns.set(context = 'notebook', style = 'whitegrid')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "XOzdawrkrxiU"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed_text.csv')\n",
    "X=df.Text\n",
    "y=df.Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below text thanks to:https://www.kaggle.com/jjjohnson/experiments-sentiment-analysis-raw-data-gru-lstm/edit\n",
    "\n",
    "https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633\n",
    "\n",
    "GloVe embeddigns thanks to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJRkd15UI5wN"
   },
   "source": [
    "Thanks to https://keras.io/examples/nlp/pretrained_word_embeddings/ for the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "total_vocabulary = word_index.keys()\n",
    "num_words = len(total_vocabulary)\n",
    "print(f'total vocabulary length: {num_words}')\n",
    "\n",
    "glove = {}\n",
    "with open('glove.840B.300d.txt',\n",
    "          'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype = np.float32)\n",
    "            glove[word] = vector\n",
    "\n",
    "\n",
    "num_tokens = len(total_vocabulary) + 2\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits +=1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(f'converted {hits} words ({misses} misses)')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(df.Text)\n",
    "X = pad_sequences(tokens)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  118,  100, 3490],\n",
       "       [   0,    0,    0, ...,   72,    6,  234],\n",
       "       [   0,    0,    0, ...,   64,   68,   54],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    1, 9379,  155],\n",
       "       [   0,    0,    0, ...,    1, 3530,  136],\n",
       "       [   0,    0,    0, ..., 2126, 1034, 2127]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "AV0p78v8ekvq"
   },
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 3095, 300)         3561300   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 3095, 1)           1208      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3095, 1)           2         \n",
      "=================================================================\n",
      "Total params: 3,562,510\n",
      "Trainable params: 1,210\n",
      "Non-trainable params: 3,561,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(X_t_glove.shape[1])))\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(1, activation='relu',\n",
    "               return_sequences = True))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 1291)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 13s 621ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 12s 584ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 13s 604ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 13s 632ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 13s 609ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 16s 764ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 16s 743ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 17s 788ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 16s 739ms/step - loss: 4.8098 - mae: 4.8098 - val_loss: 4.3462 - val_mae: 4.3462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ac41454b20>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y_t,\n",
    "         batch_size=10,\n",
    "         epochs=10,\n",
    "         validation_data=(X_val, y_val),\n",
    "      \n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN_models.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
