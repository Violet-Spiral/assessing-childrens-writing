{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5QF0aa5kOYCQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XOzdawrkrxiU"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed_text.csv')\n",
    "samples = df[['Grade','Text']]\n",
    "train_set_size = .8\n",
    "idx=np.arange(len(samples))\n",
    "np.random.shuffle(idx)\n",
    "split_point = int(len(idx)*train_set_size)\n",
    "train_samples = samples.iloc[idx[:split_point]]\n",
    "test_samples = samples.iloc[idx[split_point:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.0</td>\n",
       "      <td>In the beginning  my people were primitive  an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>6.0</td>\n",
       "      <td>More than any other state  Hawaii is world fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>3.0</td>\n",
       "      <td>One night when the air was warm, my puppies we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>I cant remember the first time I climbed that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Great people of North Dakota      I   Senator ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.0</td>\n",
       "      <td>The explosions and gunshots were definitely ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2.0</td>\n",
       "      <td>I think that Thomas Alva Edison is the most im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I will always love my grandparents beach house...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>10.0</td>\n",
       "      <td>The violin is arguably the most cherished and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>9.0</td>\n",
       "      <td>She had finally gotten her green belt in Tae K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Grade                                               Text\n",
       "65     6.0  In the beginning  my people were primitive  an...\n",
       "67     6.0  More than any other state  Hawaii is world fam...\n",
       "297    3.0  One night when the air was warm, my puppies we...\n",
       "138    6.0  I cant remember the first time I climbed that ...\n",
       "203    8.0  Great people of North Dakota      I   Senator ...\n",
       "..     ...                                                ...\n",
       "139    6.0  The explosions and gunshots were definitely ge...\n",
       "276    2.0  I think that Thomas Alva Edison is the most im...\n",
       "112    4.0  I will always love my grandparents beach house...\n",
       "318   10.0  The violin is arguably the most cherished and ...\n",
       "73     9.0  She had finally gotten her green belt in Tae K...\n",
       "\n",
       "[257 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My cat is fluffy. His name is Buzz. He is my favorite cat. Buzz is my favorite pet of all. I hug him all the time. He is soft  and his cheeks are fat like my brothers. I feel happy with my cat."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'lemma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4eacf6fc6160>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# tokens = [nlp(text) for text in df.Text]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'lemma_'"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "# tokens = [nlp(text) for text in df.Text]\n",
    "for token in tokens:\n",
    "    print(token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbEgmr80bC11"
   },
   "outputs": [],
   "source": [
    "\n",
    "feature = 'Tokens'\n",
    "X = df[feature]\n",
    "y= df.Grade\n",
    "samples = df[['Grade',feature]]\n",
    "train_set_size = .8\n",
    "idx=np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "split_point = int(len(idx)*train_set_size)\n",
    "train_samples = samples.iloc[idx[:split_point]]\n",
    "test_samples = samples.iloc[idx[split_point:]]\n",
    "\n",
    "\n",
    "vectorizer = TextVectorization(output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples.to_numpy()).batch(64)\n",
    "vectorizer.adapt(text_ds)\n",
    "# X_train=vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "# y_train = y[idx[:split_point]]\n",
    "\n",
    "# X_test=vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
    "\n",
    "# y_test = y[idx[split_point:]]\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P_bDdj-7Cmd"
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(df.Tokens, min_count=1)\n",
    "vocabulary = word2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA5njxuy7tYd"
   },
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar('school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dpv2XuKm-CwF"
   },
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJRkd15UI5wN"
   },
   "source": [
    "Thanks to https://keras.io/examples/nlp/pretrained_word_embeddings/ for the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLIXF0m-KgWs"
   },
   "outputs": [],
   "source": [
    "words = vocabulary.keys()\n",
    "word_index = dict(zip(words, range(len(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkImjtgpbSKm"
   },
   "outputs": [],
   "source": [
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vd1YA1J4bVmW"
   },
   "outputs": [],
   "source": [
    "word_index['chobot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5hyyd4oE9tH"
   },
   "outputs": [],
   "source": [
    "num_tokens = len(words)+2\n",
    "embedding_dim = len(glove_vectors['anything'])\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    try: \n",
    "        embedding_vector = glove_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    except:\n",
    "        misses +=1\n",
    "print('Converted %d words (%d misses)' % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miisqgydJ-M1"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pjAEXKIdoBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jrpAeRoaMDi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AV0p78v8ekvq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_layer=Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "int_sequences_input = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "embedded_sequences=embedding_layer(int_sequences_input)\n",
    "x=layers.LSTM(10,activation='relu',recurrent_activation='relu')(embedded_sequences)\n",
    "preds=layers.Dense(1, activation='relu')(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.asarray(X_train).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = word2vec.transform(np.array([[s] for s in df.Tokens])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "         batch_size=1,\n",
    "         epochs=10,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN_models.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
