{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5QF0aa5kOYCQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\caell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\caell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\caell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import plot_model\n",
    "\n",
    "!pip install -q -U keras-tuner\n",
    "import kerastuner as kt\n",
    "\n",
    "import IPython\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from IPython.display import display \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sns.set(context = 'notebook', style = 'whitegrid')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XOzdawrkrxiU"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed_text.csv')\n",
    "X=df.Text\n",
    "y=df.Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below text thanks to:https://www.kaggle.com/jjjohnson/experiments-sentiment-analysis-raw-data-gru-lstm/edit\n",
    "\n",
    "https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633\n",
    "\n",
    "GloVe embeddigns thanks to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocabulary length: 13727\n",
      "converted 7223 words (6504 misses)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2)\n",
    "X_t, X_val, y_val, y_val = train_test_split(X_train, y_train, random_state = 42, test_size = 0.2)\n",
    "\n",
    "total_vocabulary = set()\n",
    "for text in X_t:\n",
    "    words = text.split(' ')\n",
    "    total_vocabulary = total_vocabulary.union(words)\n",
    "num_words = len(total_vocabulary)\n",
    "print(f'total vocabulary length: {num_words}')\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_t)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt',\n",
    "          'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype = np.float32)\n",
    "            glove[word] = vector\n",
    "\n",
    "\n",
    "num_tokens = len(total_vocabulary) + 2\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits +=1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(f'converted {hits} words ({misses} misses)')\n",
    "    \n",
    "\n",
    "X_t_glove = vectorizer(np.array([[s] for s in X_t])).numpy()\n",
    "X_val_glove = vectorizer(np.array([[s] for s in X_val])).numpy()\n",
    "X_test_glove = vectorizer(np.array([[s] for s in X_test])).numpy()\n",
    "X_train_glove = vectorizer(np.array([[s] for s in X_train])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbEgmr80bC11"
   },
   "outputs": [],
   "source": [
    "\n",
    "feature = 'Tokens'\n",
    "X = df[feature]\n",
    "y= df.Grade\n",
    "samples = df[['Grade',feature]]\n",
    "train_set_size = .8\n",
    "idx=np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "split_point = int(len(idx)*train_set_size)\n",
    "train_samples = samples.iloc[idx[:split_point]]\n",
    "test_samples = samples.iloc[idx[split_point:]]\n",
    "\n",
    "\n",
    "vectorizer = TextVectorization(output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples.to_numpy()).batch(64)\n",
    "vectorizer.adapt(text_ds)\n",
    "# X_train=vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "# y_train = y[idx[:split_point]]\n",
    "\n",
    "# X_test=vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
    "\n",
    "# y_test = y[idx[split_point:]]\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P_bDdj-7Cmd"
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(df.Tokens, min_count=1)\n",
    "vocabulary = word2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA5njxuy7tYd"
   },
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar('school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dpv2XuKm-CwF"
   },
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJRkd15UI5wN"
   },
   "source": [
    "Thanks to https://keras.io/examples/nlp/pretrained_word_embeddings/ for the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLIXF0m-KgWs"
   },
   "outputs": [],
   "source": [
    "words = vocabulary.keys()\n",
    "word_index = dict(zip(words, range(len(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkImjtgpbSKm"
   },
   "outputs": [],
   "source": [
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vd1YA1J4bVmW"
   },
   "outputs": [],
   "source": [
    "word_index['chobot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5hyyd4oE9tH"
   },
   "outputs": [],
   "source": [
    "num_tokens = len(words)+2\n",
    "embedding_dim = len(glove_vectors['anything'])\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    try: \n",
    "        embedding_vector = glove_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    except:\n",
    "        misses +=1\n",
    "print('Converted %d words (%d misses)' % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miisqgydJ-M1"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pjAEXKIdoBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jrpAeRoaMDi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AV0p78v8ekvq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_layer=Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "int_sequences_input = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "embedded_sequences=embedding_layer(int_sequences_input)\n",
    "x=layers.LSTM(10,activation='relu',recurrent_activation='relu')(embedded_sequences)\n",
    "preds=layers.Dense(1, activation='relu')(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.asarray(X_train).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = word2vec.transform(np.array([[s] for s in df.Tokens])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "         batch_size=1,\n",
    "         epochs=10,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN_models.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
