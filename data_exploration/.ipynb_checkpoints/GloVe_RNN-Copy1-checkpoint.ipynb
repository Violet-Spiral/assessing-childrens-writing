{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QF0aa5kOYCQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Flatten\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import initializers, regularizers, optimizers, layers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# !pip install -q -U keras-tuner\n",
    "# import kerastuner as kt\n",
    "\n",
    "import IPython\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from IPython.display import display \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src import load_text, get_word_index\n",
    "\n",
    "\n",
    "sns.set(context = 'notebook', style = 'whitegrid')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "my6ULQUUDphy"
   },
   "source": [
    "https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633\n",
    "\n",
    "GloVe embeddigns thanks to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJRkd15UI5wN"
   },
   "source": [
    "Thanks to https://keras.io/examples/nlp/pretrained_word_embeddings/ and Kefei Mo https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633 \n",
    "\n",
    "for the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJ73328RLVIT"
   },
   "outputs": [],
   "source": [
    "df = load_text(sentences=True, grammarize=False)\n",
    "word_index = get_word_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aCd8h46Dphy"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "\n",
    "num_words = len(word_index.keys())\n",
    "print(f'total vocabulary length: {num_words}')\n",
    "\n",
    "\n",
    "num_tokens = num_words + 1\n",
    "embedding_dim = len(nlp('the').vector)\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "\n",
    "    try:\n",
    "        embedding_matrix[i+1] = nlp(word).vector\n",
    "        hits += 1\n",
    "    except:\n",
    "        misses +=1\n",
    "print(f'words converted: {hits}, words not found: {misses}')\n",
    "tokens = df.Text.apply(lambda text: [word_index[word] for word in text.split()])\n",
    "X = pad_sequences(tokens, padding='post')\n",
    "y = df.Grade\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNnFthDvLVIU"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AV0p78v8ekvq"
   },
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False)\n",
    "version = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qWnjZGsDph0"
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))    \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=.01, decay=1e-2)\n",
    "    model.compile(optimizer = adam, loss = 'mean_absolute_error', metrics = None)\n",
    "    \n",
    "    return model\n",
    "model = make_model()\n",
    "filepath = 'model1-best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_k2FvshDph0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                     batch_size=100,\n",
    "                     epochs=50,\n",
    "                     validation_split=.2,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model = keras.models.load_model('model1-best.hdf5')\n",
    "yhat = model.predict(X_test).ravel()\n",
    "print('MAE = ', np.sum(np.abs(y_test-yhat))/len(y_test))\n",
    "print('mean grade prediction = ',np.mean(model.predict(X_train)))\n",
    "print('mean grade = ', np.mean(y_train))\n",
    "\n",
    "errors = df.loc[y_test.index][['Text','Grade']]\n",
    "errors['Predicted Grade'] = yhat\n",
    "errors.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoBoQ_8-nUFx"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eussx4uJdr_T"
   },
   "outputs": [],
   "source": [
    "def make_model2():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4))) \n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=.01, decay=1e-3)\n",
    "    model.compile(optimizer = adam, loss = 'mean_absolute_error', metrics = None)\n",
    "    \n",
    "    return model\n",
    "model = make_model2()\n",
    "print(model.summary())\n",
    "filepath = 'model2-best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                     batch_size=100,\n",
    "                     epochs=50,\n",
    "                     validation_split=.2,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model = keras.models.load_model('/content/model2-best.hdf5')\n",
    "yhat = model.predict(X_test).ravel()\n",
    "print('MAE = ', np.sum(np.abs(y_test-yhat))/len(y_test))\n",
    "print('mean grade prediction = ',np.mean(model.predict(X_train)))\n",
    "print('mean grade = ', np.mean(y_train))\n",
    "\n",
    "errors = df.loc[y_test.index][['Text','Grade']]\n",
    "errors['Predicted Grade'] = yhat\n",
    "errors.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7k1n-QtboPx"
   },
   "outputs": [],
   "source": [
    "def make_model3():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu', \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4))) \n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=.01, decay=1e-3)\n",
    "    model.compile(optimizer = adam, loss = 'mean_absolute_error', metrics = None)\n",
    "    \n",
    "    return model\n",
    "model = make_model3()\n",
    "print(model.summary())\n",
    "filepath = 'model3-best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                      batch_size=100,\n",
    "#                      epochs=200,\n",
    "#                      validation_split=.2,\n",
    "#                     callbacks=callbacks)\n",
    "\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "model = keras.models.load_model('/content/model3-best.hdf5')\n",
    "yhat = model.predict(X_test).ravel()\n",
    "print('MAE = ', np.sum(np.abs(y_test-yhat))/len(y_test))\n",
    "print('mean grade prediction = ',np.mean(model.predict(X_test)))\n",
    "print('mean grade = ', np.mean(y_test))\n",
    "\n",
    "errors = df.loc[y_test.index][['Text','Grade']]\n",
    "errors['Predicted Grade'] = yhat\n",
    "errors.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Clu57TeAqQwI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmckD6uiDbfM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GloVe_RNN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
