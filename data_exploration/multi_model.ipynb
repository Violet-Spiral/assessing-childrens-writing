{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multi_model.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QF0aa5kOYCQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Flatten, \\\n",
        "Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
        "from keras.layers.experimental.preprocessing import TextVectorization\n",
        "from keras.models import Sequential, load_model\n",
        "from keras import initializers, regularizers, optimizers, layers\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.initializers import Constant\n",
        "\n",
        "\n",
        "# !pip install -q -U keras-tuner\n",
        "# import kerastuner as kt\n",
        "\n",
        "import IPython\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "from gensim.models import word2vec\n",
        "\n",
        "from IPython.display import display \n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from src import load_text, get_word_index\n",
        "\n",
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg\n",
        "\n",
        "\n",
        "sns.set(context = 'notebook', style = 'whitegrid')\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows',50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my6ULQUUDphy"
      },
      "source": [
        "https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633\n",
        "\n",
        "GloVe embeddigns thanks to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJRkd15UI5wN"
      },
      "source": [
        "Thanks to https://keras.io/examples/nlp/pretrained_word_embeddings/ and Kefei Mo https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633 \n",
        "\n",
        "for the below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ73328RLVIT"
      },
      "source": [
        "df = load_text(sentences=True, grammarize=False)\n",
        "word_index = get_word_index(text)\n",
        "X = df.Text\n",
        "y = df.Grade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0KTl112RMKF"
      },
      "source": [
        "sns.barplot(x=y.unique(),y=y.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAjaCpocSIa1"
      },
      "source": [
        "for grade in sorted(df.Grade.unique()):\r\n",
        "    text = ' '.join([story for story in df.loc[df.Grade == grade, 'Text']])\r\n",
        "    wordcloud = WordCloud().generate(text)\r\n",
        "    plt.figure(figsize=(15,12))\r\n",
        "    plt.imshow(wordcloud)\r\n",
        "    plt.title('Grade {}'.format(grade))\r\n",
        "    plt.axis('off')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptOMrHd7bFAG"
      },
      "source": [
        "X_train.str.len().max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEAiTpQIIP9Q"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2)\r\n",
        "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state = 42, test_size = 0.2)\r\n",
        "#fit the vectorizer on the text and extract the corpus vocabulary\r\n",
        "longest_sentence = X_train.str.len().max()\r\n",
        "Vectorizer = TextVectorization(output_sequence_length=longest_sentence)\r\n",
        "Vectorizer.adapt(X_train.to_numpy())\r\n",
        "vocab = Vectorizer.get_vocabulary()\r\n",
        "nlp = en_core_web_lg.load()\r\n",
        "\r\n",
        "#generate the embedding matrix\r\n",
        "num_tokens = len(vocab)\r\n",
        "embedding_dim = len(nlp('The').vector)\r\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\r\n",
        "for i, word in enumerate(vocab):\r\n",
        "    embedding_matrix[i] = nlp(word).vector\r\n",
        "\r\n",
        "#Load the embedding matrix as the weights matrix for the embedding layer and set trainable to False\r\n",
        "Embedding_layer=Embedding(\r\n",
        "    num_tokens,\r\n",
        "    embedding_dim,\r\n",
        "    embeddings_initializer=Constant(embedding_matrix),\r\n",
        "    trainable=False)\r\n",
        "\r\n",
        "#build the model.  This is a bigger one, but it works well on this problem.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdNUZObBTbZx"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\r\n",
        "model.add(Vectorizer)\r\n",
        "# model.add(Embedding_layer)\r\n",
        "model.add(Dense(256, activation='relu'))\r\n",
        "#     model.add(Dropout(rate=.2))\r\n",
        "model.add(Dense(256, activation='relu'))\r\n",
        "model.add(Dense(128, activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(128, activation='relu'))\r\n",
        "#     model.add(Dropout(rate=.2))\r\n",
        "model.add(Dense(64, activation='relu'))\r\n",
        "#     model.add(Dropout(rate=.2))\r\n",
        "model.add(Dense(64, activation='relu'))\r\n",
        "#     model.add(Dropout(rate=.2))\r\n",
        "model.add(Dense(32, activation='relu'))\r\n",
        "#     model.add(Dropout(rate=.2))\r\n",
        "model.add(Dense(1, activation='relu'))\r\n",
        "sgd = optimizers.SGD(learning_rate=.01, decay=1e-3)\r\n",
        "model.compile(optimizer=sgd, loss='mean_absolute_error')\r\n",
        "model.fit(X_train,\r\n",
        "         y_train,\r\n",
        "         epochs=200,\r\n",
        "        batch_size=100,\r\n",
        "         validation_split = .2,\r\n",
        "         verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r0K79ESKY1A"
      },
      "source": [
        "filepath = 'model-best'\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\r\n",
        "model.add(Vectorizer)\r\n",
        "model.add(embedding_layer)\r\n",
        "model.add(LSTM(100, return_sequences=True))\r\n",
        "model.add(GlobalMaxPool1D())\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(128, activation='relu', \r\n",
        "                kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(64, activation='relu', \r\n",
        "                kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(32, activation='relu', \r\n",
        "                kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))  \r\n",
        "model.add(Dropout(0.3))\r\n",
        "model.add(Dense(16, activation='relu', \r\n",
        "                kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4))) \r\n",
        "model.add(Dropout(0.3))\r\n",
        "\r\n",
        "model.add(Dense(1))\r\n",
        "\r\n",
        "adam = optimizers.Adam(learning_rate=.01, decay=1e-3)\r\n",
        "model.compile(optimizer = adam, loss = 'mean_absolute_error', metrics = None)\r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, \r\n",
        "                             save_best_only=True,\r\n",
        "                             mode='min',\r\n",
        "                             save_format='tf')\r\n",
        "callbacks = [checkpoint]\r\n",
        "\r\n",
        "history = model.fit(X_train, y_train,\r\n",
        "                     batch_size=100,\r\n",
        "                     epochs=200,\r\n",
        "                     validation_split=.2,\r\n",
        "                    callbacks=callbacks\r\n",
        "                    )\r\n",
        "\r\n",
        "plt.plot(history.history['loss'], label='Training Loss')\r\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "model = keras.models.load_model(filepath)\r\n",
        "yhat = model.predict(X_test).ravel()\r\n",
        "print('MAE = ', np.sum(np.abs(y_test-yhat))/len(y_test))\r\n",
        "print('mean grade prediction = ', np.mean(model.predict(X_test)))\r\n",
        "print('mean grade = ', np.mean(y_test))\r\n",
        "\r\n",
        "errors = df.loc[y_test.index][['Text','Grade']]\r\n",
        "errors['Predicted Grade'] = yhat\r\n",
        "errors.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-F4xh4dME2D"
      },
      "source": [
        "model.save('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHurGEsKODal"
      },
      "source": [
        "modeltest = keras.models.load_model('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCrdQiFLOW7I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}